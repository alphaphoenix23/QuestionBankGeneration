{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Introduction</p> <p>Question paper generation is a critical aspect of educational assessment, influencing the effectiveness and fairness of examinations. Crafting well-structured and diverse sets of questions requires careful consideration of various factors, including the educational objectives, difficulty levels, and coverage of the curriculum. In recent years, advancements in technology have transformed the landscape of question paper generation. Traditional methods often relied on manual creation, a time-consuming and labor-intensive process. However, with the integration of artificial intelligence and machine learning techniques, there has been a paradigm shift towards automated question paper generation. This introduction explores the evolving landscape of question paper generation, delving into the current methods and technological innovations that contribute to more efficient, dynamic, and customized examination assessments.</p> <p>Problem Statement</p> <p>Given the curriculum for an exam as input, the question paper generator must provide multiple question papers and answers, each with valid questions with answers bound by the curriculum. The professor should be able to interact with the platform, upload syllabus content and review question papers and their answers.</p> <p>Approach</p> <p>The approach to the problem statement consists of the following steps:</p> <ol> <li> <p>Data Collection and Preparation</p> </li> <li> <p>Building RAG framework</p> </li> <li> <p>Testing and Evaluation of Model</p> </li> <li> <p>User Interface Platform Development</p> </li> <li> <p>Testing UI Platform</p> </li> </ol> <p>Data Collection and Preparation</p> <ol> <li> <p>A domain-specific archive will be established for a standard method     &gt; of data collection. Professors will be required to submit past     &gt; question papers, evaluation schemes, course content, etc to the     &gt; archive.</p> </li> <li> <p>The archive will be accessible through a Typescript based website     &gt; where data can be uploaded, reviewed and edited.</p> </li> <li> <p>This data will be organized by time and domains and will be     &gt; available for filtering and sorting through various features.</p> </li> <li> <p>The data will be further used for RAG.</p> </li> </ol> <p>Building RAG Framework</p> <p>RAG is an AI framework that improves the accuracy and reliability of large language models (LLMs) by grounding them in external knowledge bases. LLMs can be inconsistent and prone to errors, lacking true understanding of word meaning. RAG addresses these issues by providing access to up-to-date facts and verifiable sources, increasing user trust.</p> <ol> <li> <p>An unstructured Database is generated in the form of Vector     &gt; databases by improving on existing techniques of generating     &gt; embeddings from text.</p> </li> <li> <p>A knowledge graph, or a structured database, is generated by     &gt; improving on existing techniques to plot entity relationships.</p> </li> <li> <p>These databases are collectively plugged into the Question and     &gt; Answer Generation model (LLM).</p> </li> <li> <p>User Input as natural language is processed to query the databases     &gt; and the input, along with retrieved data, is used by the LLM to     &gt; generate output.</p> </li> </ol> <p>Testing and Evaluation of Model</p> <ol> <li> <p>Above established architecture is then tested in multiple phases,     &gt; both automated and manual, to ensure lack of potential errors and     &gt; unexpected responses.</p> </li> <li> <p>It is then evaluated using qualitative methods (such as checking for     &gt; hallucinations) and quantitative methods (such as accuracy     &gt; metrics).</p> </li> <li> <p>Lastly, the framework is subjected to user testing through a review     &gt; process with faculty.</p> </li> </ol> <p>User Interface Platform Development</p> <p>The Framework will be accessible to all faculty through a platform consisting of a dashboard with question paper metrics, Import and export options for curriculum content and question answer sets respectively, and a conversational bot for any user input or queries.</p> <p>The platform will be developed using JavaScript and Python.</p> <p>Testing UI Platform</p> <p>Lastly, the UI platform will be thoroughly tested in stages, during and after development.</p> <ol> <li> <p>Unit testing, Regression testing will be carried out throughout the     &gt; development process.</p> </li> <li> <p>Environment testing will be performed post development to ensure     &gt; proper functionality in user environment.</p> </li> <li> <p>The platform will then be reviewed by faculty for beta testing and     &gt; any necessary changes.</p> </li> </ol> <p>LLM Used for Task</p> <p>The LLM used for above task will be an improved version of [Question and Answer Generator (QAG)]{.underline}.</p> <p>Conclusion</p> <p>In conclusion, this comprehensive approach to question paper generation, encompassing data collection, the development of the RAG framework, testing and evaluation, user interface platform creation, and the utilization of a powerful Language Model (LLM), represents a significant stride towards revolutionizing the educational assessment landscape. The integration of cutting-edge technologies, such as artificial intelligence and machine learning, is poised to enhance the efficiency and customization of examination assessments. The RAG framework, with its grounding in external knowledge bases and its ability to improve the accuracy of large language models, holds promise in mitigating the inconsistencies and errors often associated with traditional question paper generation. The user interface platform, designed with the faculty in mind, ensures a user-friendly experience, allowing seamless interaction with the system for curriculum uploads, question paper reviews, and more. Through rigorous testing and evaluation, both automated and user-centric, the reliability of the framework is affirmed, paving the way for a transformative tool in the hands of educators. As this approach unfolds, it promises to streamline and elevate the process of question paper generation, ultimately contributing to more effective and fair educational assessments.</p>"},{"location":"Fine-Tuning%20Vs.%20RAG/","title":"Fine Tuning Vs. RAG","text":"<p>Knowledge Graphs &amp; LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation</p> <p>What are the limitations of LLMs, and how to overcome them</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney's idea of a knowledge graph chatbot.</p> <p>This is the second blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase\u00a0[practical uses of these LLMs in conjunction with Neo4j]{.underline}. As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can find the first and third blog posts here:</p> <ul> <li> <p>[Harnessing LLMs With     Neo4j]{.underline}</p> </li> <li> <p>[Multi-Hop Question     Answering]{.underline}</p> </li> </ul> <p>Harnessing Large Language Models with Neo4j</p> <p>Episode 1 --- Exploring Real-World Use Cases</p> <p>medium.com</p> <p>The first wave of hype for Large Language Models (LLMs) came from ChatGPT and similar web-based chatbots, where the models are so good at understanding and generating text that it shocked people, myself included.</p> <p>Many of us logged in and tested its ability to write haikus, motivational letters, or email responses. What became quickly apparent is that LLMs are not only good at generating creative context but also at solving typical natural language processing and other tasks.</p> <p>Shortly after the LLM hype started, people started considering integrating it into their applications. Unfortunately, if you simply develop a wrapper around an LLM API, there is a high chance your application will not be successful as it doesn't provide additional value.</p> <p>One major problem of LLMs is the so-called\u00a0knowledge cutoff. The knowledge cutoff term indicates that LLMs are\u00a0unaware of any events that happened after their training. For example, if you ask ChatGPT about an event in 2023, you will get the following response.</p> <p>{width=\"6.268055555555556in\" height=\"2.292361111111111in\"}</p> <p>ChatGPT's knowledge cutoff date. Image by author.</p> <p>The same problem will occur if you ask an LLM about\u00a0any event not present\u00a0in its training dataset. While the knowledge cutoff date is relevant for any publicly available information, the LLM doesn't have any knowledge about\u00a0private or confidential information\u00a0that might be available even before the knowledge cutoff date.</p> <p>For example, most companies have some confidential information that they don't share publicly but might be interested in having a custom LLM that could answer those questions. On the other hand, a lot of the publicly available information that the LLM is aware of might be already outdated.</p> <p>Therefore, updating and expanding the knowledge of an LLM is highly relevant today.</p> <p>Another problem with LLMs is that they are trained to produce realistic-sounding text, which\u00a0might not be accurate. Some invalid information is more challenging to spot than others. Especially for missing data, it is very probable that the LLM will make up an answer that sounds convincing but is nonetheless wrong instead of admitting that it lacks the base facts in its training.</p> <p>For example, research or court citations might be easier to verify. A week ago, a\u00a0[lawyer got in trouble for blindly believing the court citations ChatGPT produced]{.underline}.</p> <p>Lawyer apologizes for fake court citations from ChatGPT | CNN Business</p> <p>The meteoric rise of ChatGPT is shaking up multiple industries - including law. A lawyer for a man suing Avianca...</p> <p>edition.cnn.com</p> <p>I have also noticed that LLMs will consistently\u00a0produce assertive, yet false information about any sort of IDs\u00a0like the WikiData or other identification numbers.</p> <p>{width=\"6.268055555555556in\" height=\"3.5180555555555557in\"}</p> <p>ChatGPT's hallucinations. Image by author.</p> <p>Since the response by ChatGPT is assertive, you might expect it to be accurate. However, the given WikiData id points to a farm in England. Therefore, you have to be very careful not to blindly believe everything that LLMs produce. Verifying answers or producing more accurate results from LLMs is another big problem that needs to be solved.</p> <p>Of course, LLMs have other problems, like bias, prompt injection, and others. However, we will not talk about them here. Instead, in this blog post, we will present and focus on the\u00a0concepts of fine-tuning and retrieval-augmented LLMs\u00a0and evaluate their pros and cons.</p> <p>Supervised Fine-Tuning of an LLM</p> <p>Explaining how LLMs are trained is beyond the scope of this blog post. Instead, you can watch this\u00a0[incredible video by Andrej Karpathy to catch up on LLMs]{.underline}\u00a0and learn about the different phases of LLM training.</p> <p>By fine-tuning an LLM, we refer to the\u00a0supervised training phase, during which you provide additional question-answer pairs to optimize the performance of the Large Language Model (LLM).</p> <p>Additionally, we have identified two different use cases for fine-tuning an LLM.</p> <p>One use case\u00a0is fine-tuning a model to update and\u00a0expand its internal knowledge.\\ In contrast, the\u00a0other use case\u00a0is focused on fine-tuning a model\u00a0for a specific\u00a0task like text summarization or translating natural language to database queries.</p> <p>First, we will talk about the first use case, where we use fine-tuning techniques to update and expand the internal knowledge of an LLM.</p> <p>{width=\"6.268055555555556in\" height=\"3.1590277777777778in\"}</p> <p>Supervised fine-tuning flow. Image by author. Icons from\u00a0[Flaticons]{.underline}.</p> <p>Usually, you want to avoid pre-training an LLM as the\u00a0cost\u00a0can be upwards of hundreds of thousands and even millions of dollars. A base LLM is pre-trained using a gigantic corpus of text corpus, frequently in the billions or even trillions of tokens.</p> <p>While the\u00a0number of parameters\u00a0of an LLM is vital, it is not the only parameter you should consider when selecting a base LLM. Besides the\u00a0license, you should also consider the\u00a0bias\u00a0and\u00a0toxicity\u00a0of the pre-training dataset and the base LLM.</p> <p>After you have selected the base LLM, you can start the next step of fine-tuning it. The fine-tuning step is relatively cheap regarding computation cost due to available techniques like the\u00a0[LoRa]{.underline}\u00a0and\u00a0[QLoRA]{.underline}.</p> <p>Using LoRA for Efficient Stable Diffusion Fine-Tuning</p> <p>LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal...</p> <p>huggingface.co</p> <p>However,\u00a0constructing a training dataset\u00a0is more complex and can get expensive. If you can not afford a dedicated team of annotators, it seems that the trend is to\u00a0use an LLM to construct a training dataset\u00a0to fine-tune your desired LLM (this is really meta).</p> <p>For example,[\u00a0Stanford's Alpaca training dataset was created using OpenAI's LLMs]{.underline}. The cost to produce 52 thousand training instructions was about 500 dollars, which is relatively cheap.</p> <p>Stanford CRFM</p> <p>We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our...</p> <p>crfm.stanford.edu</p> <p>On the other hand, the\u00a0[Vicuna model was fine-tuned by using the ChatGPT conversations users posted on ShareGPT.com]{.underline}.</p> <p>Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality | LMSYS Org</p> <p>by: The Vicuna Team, Mar 30, 2023 We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on...</p> <p>lmsys.org</p> <p>There is also a relatively\u00a0[fresh project by H2O called WizardLM]{.underline}, which is designed to turn documents into question-answer pairs that can be used to fine-tune an LLM.</p> <p>We haven't found any recent articles describing how to use a knowledge graph to prepare good question-answer pairs that can be used to fine-tune an LLM.</p> <p>This is an area that we plan to explore during the NaLLM project. We have some ideas for utilizing LLMs to construct question-answer pairs from a knowledge graph context.</p> <p>However, there are a lot of unknowns at the moment.\\ For example, can you provide\u00a0two different answers\u00a0to the same question, and the LLM then somehow combines them in its internal knowledge store?</p> <p>Another consideration is that some information in a knowledge graph is not relevant without considering its relationships. Therefore, do we have to pre-define relevant queries, or is there a more generic way to go about it? Or can we use the node-relationship-node patterns representing subject-predicate-object expressions to generate relevant pairs?</p> <p>These are some of the questions we aim to answer in upcoming blog posts.</p> <p>Imagine that you somehow managed to produce a training dataset containing question-answer pairs based on the information stored in your knowledge graph. As a result, the LLM now includes updated knowledge.</p> <p>However, fine-tuning the model didn't solve the knowledge cutoffs problem since it only pushed the knowledge cutoff to a later date.</p> <p>Therefore, we recommend updating the internal knowledge of an LLM through fine-tuning techniques\u00a0only for slowly changing\u00a0or updating data. For example, you could use a fine-tuned model to provide tourist information.</p> <p>However, you would run into troubles the second you would want to include special time-dependent (real-time) or personalized promotions in the responses. Similarly, fine-tuned models are not ideal for analytical workflows where you would ask how many new customers the company gained over the last week.</p> <p>At the moment,\u00a0fine-tuning approaches can help mitigate hallucinations\u00a0but cannot completely eliminate them. One problem is that the LLMs\u00a0do not cite their sources\u00a0when providing answers. Therefore, you have no idea if the answer came from pre-training data, fine-tuning dataset, or was made up by the LLM. Additionally, there might be another possible falsehood source if you use an LLM to create the fine-tuning dataset.</p> <p>Lastly, a fine-tuned model cannot automatically provide different responses\u00a0depending on the user\u00a0making the questions. Likewise, there is no concept of access restrictions, meaning that anybody interacting with the LLM has access to all of its information.</p> <p>Retrieval-Augmented Generation</p> <p>Large language models perform remarkably well in natural language applications like</p> <ul> <li> <p>Text summarization,</p> </li> <li> <p>Extracting relevant information,</p> </li> <li> <p>Disambiguation of entities</p> </li> <li> <p>Translating from one language to another, or even</p> </li> <li> <p>Converting natural language into database queries or scripting code.</p> </li> </ul> <p>Moreover, previously NLP models were most often domain and task-specific, meaning that you would most likely need to train a custom natural language model depending on your use case and domain. However, thanks to the generalization capabilities of LLMs, a single model can be applied to solve various collections of tasks.</p> <p>We have observed quite a strong trend in using retrieval-augmented LLMs, where instead of using LLMs to access its internal knowledge, you use the LLM\u00a0as a natural language interface\u00a0to your company's or private information.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Retrieval-augmented generation. Image by author. Icons from\u00a0[Flaticons]{.underline}.</p> <p>The retrieval augmented approach uses the LLM to generate an answer based on the additionally provided relevant documents from your data source.</p> <p>Therefore, you don't rely on internal knowledge of the LLM to produce answers. Instead, the LLM is used only for extracting relevant information from documents you passed in and summarizing it.</p> <p>ChatGPT plugins</p> <p>openai.com</p> <p>For example, the\u00a0[ChatGPT plugins]{.underline}\u00a0can be thought of as a retrieval-augmented approach to LLM applications. The ChatGPT interface with a browsing plugin enabled allows the LLM to search the internet to access up-to-date information and use it to construct the final answer.</p> <p>{width=\"6.268055555555556in\" height=\"5.6090277777777775in\"}</p> <p>ChatGPT with browsing plugin. Image by author.</p> <p>In this example, ChatGPT was able to answer who won the Oscar for various categories in 2023. But, remember, the cutoff knowledge date for ChatGPT is 2021, so it couldn't know who won the 2023 Oscars from its internal knowledge. Therefore, it accessed external information through the browsing plugin, which allowed it to answer the question with up-to-date information. Those plugins present an integrated augmentation mechanism inside the OpenAI platform.</p> <p>If you have been watching the LLM space, you might have heard of the\u00a0[LangChain library]{.underline}.</p> <p>Getting Started with LangChain: A Beginner's Guide to Building LLM-Powered Applications</p> <p>A LangChain tutorial to build anything with large language models in Python</p> <p>towardsdatascience.com</p> <p>The LangChain library can be used to allow LLMs to access real-time information from various sources like Google Search, vector databases, or knowledge graphs. For example, LangChain has added a\u00a0[Cypher Search chain]{.underline}, which converts natural language questions into a Cypher statement, uses it to retrieve information from the Neo4j database, and constructs a final answer based on the provided information.</p> <p>LangChain has added Cypher Search</p> <p>With the LangChain library, you can conveniently generate Cypher queries, enabling an efficient retrieval of...</p> <p>towardsdatascience.com</p> <p>With the Cypher Search chain, an LLM is not only used to construct a final answer but also to translate a natural language question into a Cypher query.</p> <p>{width=\"6.268055555555556in\" height=\"3.761111111111111in\"}</p> <p>Cypher search in LangChain. Image by author.</p> <p>Another popular library for retrieval-augmented LLM workflows is\u00a0[LlamaIndex (GPT Index)]{.underline}. LlamaIndex is a comprehensive data framework aimed at enhancing the performance of Large Language Models (LLMs) by enabling them to leverage private or custom data.</p> <p>LlamaIndex on TWIML AI: A Distilled Summary (using LlamaIndex)</p> <p>Overview</p> <p>medium.com</p> <p>Firstly, LlamaIndex offers data connectors that facilitate the ingestion of a variety of data sources and formats, encompassing everything from APIs, PDFs, and documents to SQL or graph data.</p> <p>This feature allows for an effortless integration of existing data into the LLM. Secondly, it provides efficient mechanisms to structure the ingested data using indices and graphs, ensuring the data is suitably arranged for use with LLMs. In addition, it includes an advanced retrieval and query interface, which enables users to input an LLM prompt and receive back a context-retrieved, knowledge-augmented output.</p> <p>The idea behind retrieval-augmented LLM applications like ChatGPT Plugins and LangChain is to avoid relying on internal LLM knowledge only to generate answers. Instead, LLMs are used to solve tasks like\u00a0[constructing database queries from natural language]{.underline}\u00a0and constructing answers based on externally provided information or by utilizing plugins/agents for retrieval.</p> <p>The retrieval-augmented approach has some clear advantages over the fine-tuning approach:</p> <ul> <li> <p>The answer can cite its sources of information, which allows you to     validate the information and potentially change or update the     underlying information based on requirements</p> </li> <li> <p>Hallucinations are more unlikely to occur as you don't rely on the     internal knowledge of an LLM to answer the question and only use     information that is provided in the relevant documents</p> </li> <li> <p>Changing, updating, and maintaining the underlying information the     LLM uses is easier as you transform the problem from LLM maintenance     to a database maintenance, querying and context construction problem</p> </li> <li> <p>Answers can be personalized based on the user context, or their     access permission</p> </li> </ul> <p>On the other hand, you should consider the following limitations when using the retrieval-augmented approach:</p> <ul> <li> <p>The answers are only as good as the smart search tool</p> </li> <li> <p>The application needs access to your specific knowledge base, either     that be a database or other data stores</p> </li> <li> <p>Completely disregarding the internal knowledge of the language model     limits the number of questions that can be answered</p> </li> <li> <p>Sometimes LLMs fail to follow instructions, so there is a risk that     the context might be ignored or hallucinations occur if no relevant     answer data is found in the context.</p> </li> </ul> <p>Summary</p> <p>This blog post delves into the limitations of Large Language Models (LLMs), such as</p> <ul> <li> <p>Knowledge cutoff,</p> </li> <li> <p>Hallucinations, and</p> </li> <li> <p>The lack of user customization.</p> </li> </ul> <p>To overcome these, we explored two concepts, namely, fine-tuning and retrieval-augmented use of LLMs.</p> <p>Fine-tuning an LLM\u00a0involves the supervised training phase, where question-answer pairs are provided to optimize the performance of the LLM. This can be used to update and expand the LLM's internal knowledge or fine-tune it for a specific task. However, fine-tuning fails to solve the knowledge cutoff issue as it simply pushes the cutoff to a later date. It also cannot fully eliminate hallucinations. Therefore, we recommend using the fine-tuning approach for slowly changing datasets where some hallucinations are allowed. Since fine-tuning LLMs is relatively new, we are eager to learn more about fine-tuning approaches and best practices.</p> <p>The second approach to overcome the limitations of LLMs is the so-called\u00a0retrieval-augmented generation, where the LLM serves as a natural language interface to access external information, thereby not relying only on its internal knowledge to produce answers. Advantages of the retrieval-augmented approach include source-citing, negligible hallucinations, ease of changing and updating information, and personalization.\\ However, it relies heavily on the intelligent search tool to retrieve relevant information and requires access to the user's knowledge base. Furthermore, it can only answer queries provided it has the information required to address the question.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p> <p>GitHub - neo4j/NaLLM: Repository for the NaLLM project</p> <p>Welcome to the NaLLM project repository, where we are exploring and demonstrating the synergies between Neo4j and Large...</p> <p>github.com</p>"},{"location":"Harnessing%20LLMs%20with%20Neo4j/","title":"Harnessing LLMs with Neo4j","text":"<p>Knowledge Graphs &amp; LLMs: Harnessing Large Language Models with Neo4j</p> <p>Exploring Real-World Use Cases</p> <p>This is the first blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can read the later blog posts here:</p> <ul> <li> <p>[Fine-Tuning vs Retrieval-Augmented     Generation]{.underline}</p> </li> <li> <p>[Multi-Hop Question     Answering]{.underline}</p> </li> </ul> <p>Large Language Models (LLMs) like ChatGPT have taken the world by storm in 2023 due to their ability to understand and generate human-like text. Their capacity to adapt to different conversational contexts, answer questions across a wide range of topics, and even simulate creative writing has revolutionized the way humans and machines interact, sparking a new wave of artificial intelligence applications.</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Steampunk computer wall with a magic mirror operated by ants running in transparent tubes (Midjourney)</p> <p>Thanks to their ability to \"understand\", generate, and refine human-like text, LLMs offer us new methods for working with data. Our team at Neo4j has started a project to\u00a0explore, develop,\u00a0and\u00a0showcase\u00a0practical uses of these LLMs in conjunction with Neo4j.</p> <p>One key aspect of this project is the integration of graph database technology and concepts into the LLM application stack. By doing so, we expect to enhance the\u00a0accuracy, transparency,\u00a0and\u00a0predictability\u00a0of the model output and open up new use-cases both for using LLMs as well as databases.</p> <p>As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute.</p> <p>While we base our project on our current understanding and technology, we fully acknowledge that this is a rapidly advancing field, and future findings may refine our approach. Consequently, our perspective and strategies are subject to change in response to new data and technological progress.</p> <p>Identifying the Real-World Use Cases</p> <p>The initiation phase of our project focused on the identification of real-world use cases, which would form the basis for our upcoming solutions. After thorough research, market analysis, and customer interactions, we've narrowed down\u00a0two initial use cases\u00a0that frequently feature in our conversations with customers.</p> <p>1. Natural Language Interface to a Knowledge Graph</p> <p>Our first use case focuses on developing a natural language interface for knowledge graphs. The goal is to create a user interface that simplifies the process of data selection, querying and processing, making data more accessible and easier to understand.</p> <p>Allowing you to \"talk to your database\".</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney imagination of a natural language interface for a KG</p> <p>The preferred method for this is a chat-like interface that would\u00a0generate database queries\u00a0based on the user question and the inferred schema of the database.</p> <p>Based on feedback from our users, there is a significant demand for\u00a0natural language responses\u00a0over just citing data and linking to sources. By utilizing LLMs, we can provide these responses, presenting information in a way that mimics natural, human conversation.</p> <p>We're exploring techniques to inform the LLMs about the content of the knowledge graph. This could involve a similarity search on vectorized content passed via context or fine-tuning a model on the knowledge graph itself.</p> <p>{width=\"6.268055555555556in\" height=\"6.060416666666667in\"}</p> <p>Example sequence diagram of a NL interface to KG solution could look like</p> <p>However, while simplicity and comprehensibility are important, so too are the\u00a0accuracy and credibility of information. To ensure this, all responses should include\u00a0links to source data, offering full transparency and traceability.</p> <p>These advancements for LLMs and their integration into knowledge graph interfaces represent an exciting step forward in making complex data more user-friendly and trustworthy.</p> <p>2. Creating a Knowledge Graph From Unstructured Data</p> <p>The second use case showcases the creation of knowledge graphs from a multitude of unstructured data sources, including but not limited to PDFs, HTML pages, and text documents.</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney has no good concepts of sieves or funnels transforming information :)</p> <p>LLMs interpret various types and meanings in the text, making sense of unstructured data by identifying its inherent structure based on the training data.</p> <p>They can</p> <ul> <li> <p>decipher entities,</p> </li> <li> <p>discern relationships, and</p> </li> <li> <p>eliminate redundancies by recognizing duplicates.</p> </li> </ul> <p>In effect, LLMs can transform a seemingly indistinguishable mass of unstructured text into a well-organized, meaningful knowledge graph of entities and their relationships.</p> <p>{width=\"6.268055555555556in\" height=\"3.8465277777777778in\"}</p> <p>Example sequence diagram of KG creation could look like</p> <p>Interestingly you can guide LLMs with the appropriate prompts to output structured data directly, e.g. as JSON data structures for node- and relationship-lists, that we can feed directly into the graph database.</p> <p>By leveraging LLMs in this way, we can streamline the knowledge graph creation process, improving efficiency and accuracy. Especially the disambiguation helps with the many variants we humans put into our texts for the same entities and relationships just to entertain the reader.</p> <p>As a result, valuable data becomes easier to access, understand, and use for decision-making.</p> <p>Next Steps</p> <p>As the project progresses, our goal will be to develop prototypes for these use cases. We aim to improve the interaction between you, the users, and your connected data using Neo4j and LLMs.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository:\u00a0[https://github.com/neo4j/NaLLM]{.underline}</p>"},{"location":"Knowledge%20Graphs%20From%20Unstructured%20Text/","title":"Knowledge Graphs From Unstructured Text","text":"<p>Construct Knowledge Graphs From Unstructured Text</p> <p>This is the fifth blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we have constructed and publicly displayed demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in a blog post. You can read the previous four blog posts here:</p> <ul> <li> <p>[Harnessing LLMs with     Neo4j]{.underline}</p> </li> <li> <p>[Fine-tuning vs. Retrieval-augmented     generation]{.underline}</p> </li> <li> <p>[Multi-hop Question     Answering]{.underline}</p> </li> <li> <p>[Knowledge Graphs &amp; LLMs: Real-Time Graph     Analytics]{.underline}</p> </li> </ul> <p>This blog post will explore a use case we investigated during our project: extracting information from unstructured data. Organizations have long faced challenges in extracting meaningful insights from unstructured data. Such data encompasses textual content, images, audio, and other non-tabular formats, holding immense potential yet often remaining difficult to use due to its inherent complexity. Our primary focus in this post will be to extract information from unstructured text by converting it into nodes and relationships.</p> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Illustration from\u00a0[Imagine.art]{.underline}\u00a0of \"extracting information from unstructured data\"</p> <p>Recent years have witnessed significant advancements in natural language processing techniques, revolutionizing the transformation of unstructured data into valuable knowledge. With the emergence of powerful language models like OpenAI's GPT models and leveraging the power of machine learning, the process of converting unstructured text data into structured representations has become more accessible and efficient.</p> <p>One such representation is\u00a0knowledge graphs,\u00a0which offer a robust framework for representing complex relationships and connections among various entities. They provide a structured representation of the data, enabling intuitive querying and exploration of the information contained within. This structured nature allows for advanced semantic analysis, reasoning, and inference, facilitating more accurate and comprehensive decision-making processes.</p> <p>{width=\"5.066666666666666in\" height=\"4.908333333333333in\"}</p> <p>Example of Knowledge Extraction Pipeline</p> <p>We will explore how Large Language Models (LLMs) have simplified the conversion of unstructured data into knowledge graphs, using an approach that utilizes the language skills of LLMs to perform nearly all parts of the process. The process can be divided into three steps:</p> <ol> <li> <p>Extracting nodes and edges</p> </li> <li> <p>Entity disambiguation</p> </li> <li> <p>Importing into Neo4j</p> </li> </ol> <p>Let's walk through each of these steps:</p> <p>1. Extracting nodes and relationships:\u00a0To tackle this problem, we take the simplest possible approach, where we pass the input data to the LLM and let it decide which nodes and relationships to extract. We ask the LLM to return the extracted entities in a specific format, including a name, a type, and properties. This allows us to extract nodes and edges from the input text.</p> <p>However, LLMs have a limitation known as the\u00a0context window\u00a0(between 4 and 16,000 tokens for most LLMs), which can be easily overwhelmed by larger inputs, hindering the processing of such data. To overcome this limitation, we employ a strategy of dividing the input text into smaller, more manageable chunks that fit within the context window.</p> <p>Determining the optimal splitting points for the text is a challenge of its own. To keep things simple, we have chosen to divide the text into chunks of maximum size, maximizing the utilization of the context window per chunk. Additionally, we introduce some overlap from the previous chunk to account for cases where a sentence or description spans across multiple chunks. This approach allows us to extract nodes and edges from each chunk, representing the information contained within it.</p> <p>To maintain consistency in the labeling of different types of entities across chunks, we provide the LLM with a list of node types that were extracted in the previous chunks. Those start forming the extracted \"schema.\" We have observed that this approach enhances the uniformity of the final labels. For example, instead of the LLM generating separate types for \"Company\" and \"Gaming Company,\" it consolidates all types of companies under a \"Company\" label.</p> <p>One\u00a0notable hurdle\u00a0in our approach is the problem of duplicate entities. Since each chunk is processed semi-independently, information about the same entity found in different chunks will create duplicates when we combine the results. Naturally, this issue brings us to our next step.</p> <p>2. Entity disambiguation:\u00a0We now have a set of entities. To address the issue of duplication, we employ LLMs once again. First, we organize the entities into sets based on their type. Subsequently, we provide each set to the LLM, enabling it to merge duplicate entities while simultaneously consolidating their properties. We use LLMs for this since we don't know what name each entity has been given. For example, the initial extraction could have ended up with two nodes: (Alice {name: \"Alice Henderson\"}) and (Alice Henderson {age: 25}). These reference the same entity and should be merged to a single node with both the name and age property. We use LLMs to accomplish this since it's great at quickly understanding which nodes actually reference the same entity.</p> <p>By iteratively performing this procedure for all entity groups, we obtain a structured data set that is ready for further processing.</p> <p>3. Importing the data into Neo4j:\u00a0In the final step of the process, we focus on importing the results we got from the LLM into a Neo4j database. This requires a format that Neo4j can understand. To accomplish this, we parse the generated text from the LLM and transform it into separate CSV files, corresponding to the various node and relationship types. These CSV files are subsequently mapped to a format compatible with the\u00a0[Neo4j Data Importer tool]{.underline}. Through this conversion, we gain the advantage of previewing the data before initiating the import process into a Neo4j database, harnessing the capabilities offered by the Neo4j Importer tool.</p> <p>{width=\"6.268055555555556in\" height=\"3.323611111111111in\"}</p> <p>Overview of the application</p> <p>Putting this all together, we have created an application consisting of three parts: a UI to input a file, a controller that executes the previously explained process, and an LLM that the controller talks to. This demo application can be found\u00a0[here]{.underline}, and the source code can be found on\u00a0[GitHub]{.underline}.</p> <p>We also created a version of this pipeline that works essentially in the same way but with the option to include a schema. This schema works like a filter where the user can restrict which types of nodes and relationships and which properties the LLM should include in its result.</p> <p>NaLLM Graph Construction Demo</p> <p>nallm-experiments.ew.r.appspot.com</p> <p>If you are interested in learning more about generative AI and knowledge graphs, I would suggest taking a look at\u00a0[Neo4j's page about generative AI]{.underline}.</p> <p>Demonstration</p> <p>I tested the application by giving it the Wikipedia page for the\u00a0[James Bond franchise]{.underline}\u00a0and inspected the generated knowledge graph.</p> <p>{width=\"6.268055555555556in\" height=\"3.688888888888889in\"}</p> <p>Example of the resulting graph</p> <p>The provided graph subset showcases the generated graph, which, in my opinion, provides a reasonably accurate depiction of the Wikipedia article. The graph primarily consists of nodes representing books and individuals associated with those books, such as authors and publishers.</p> <p>However, there are a few issues with the graph. For instance, Ian Fleming is labeled as a publisher rather than an author for most of the books he wrote. This discrepancy may be attributed to the difficulty the language model had in comprehending that particular aspect of the Wikipedia article.</p> <p>Another problem is the inclusion of relationships between book nodes and the directors of films with the same titles, instead of creating separate nodes for the movies.</p> <p>Finally, It's worth noting that the LLM appears to be quite literal in its interpretation of relationships, as evidenced by using the relationship type \"used\" to connect the James Bond character with the cars he drove. This literal approach may stem from the article's usage of the verb \"used\" rather than \"drove.\"</p> <p>A full video of the demonstration can be found here:</p> <p>Demo of KG Construction</p> <p>Problems</p> <p>For a demonstration, this approach worked fairly well, and we think it shows that it's possible to use LLMs to create knowledge graphs. However, we acknowledge certain issues need to be addressed within this approach:</p> <ul> <li> <p>Unpredictable output:\u00a0This is inherent to the nature of LLMs. We     do not know how an LLM will format its results. Even if we ask it to     output in a specific format, it might not obey. This might cause     problems when trying to parse what it generates. We saw one instance     of this while chunking the data: Most of the time, the LLM generated     a simple list of nodes and edges, but sometimes the LLM would number     the list. Tools to work around this are starting to be released,     such     as\u00a0[Guardrails]{.underline}\u00a0and\u00a0[OpenAIs     Function API]{.underline}. It's still early in the world of LLM     tooling, so we anticipate that this will not be a problem for long.</p> </li> <li> <p>Speed: This approach is slow and often takes several minutes for     just a single reasonably large web page. There might be a     fundamentally different approach that can make the extraction go     faster.</p> </li> <li> <p>Lack of accountability: There is no way of knowing why the LLM     decided to extract some information from the source documents or if     the information even exists in the source. The data quality of the     resulting knowledge graph is, therefore, much lower than the graph     created by processes not leveraging LLMs.</p> </li> </ul> <p>Summary</p> <p>This blog post explored a use case of Large Language Models with Neo4j to extract insights from unstructured data by converting it into a structured representation in the form of a knowledge graph.</p> <p>We discussed a three-step approach focusing on extracting nodes and relationships, entity disambiguation, and importing the data into Neo4j. By utilizing LLMs, anyone can automate the extraction process and efficiently process large amounts of unstructured data.</p> <p>However, there are challenges\u00a0to address, including unpredictable output formatting, speed limitations, and the lack of accountability. Despite these issues, the combined power of LLMs and Neo4j offers a promising solution for unlocking the hidden value in unstructured data, even for non-technical users.</p>"},{"location":"Multi-Hop/","title":"Multi Hop","text":"<p>Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</p> <p>Retrieve information that spans across multiple documents</p> <p>third blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these\u00a0[LLMs in conjunction with Neo4j]{.underline}. As part of this project, we will construct and publicly display demonstrations in a\u00a0[GitHub repository]{.underline}, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can see the previous two blog posts here:</p> <ul> <li> <p>[Harnessing LLMs With     Neo4j]{.underline}</p> </li> <li> <p>[Fine-Tuning vs Retrieval-Augmented     Generation]{.underline}</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Midjourney's idea of an investigative board.</p> <p>In the[\u00a0previous blog post]{.underline}, we learned about the retrieval-augmented approach to overcome the limitations of Large Language Models (LLMs), such as hallucinations and limited knowledge. The idea behind the retrieval-augmented approach is to reference external data at question time and feed it to an LLM to enhance its ability to generate accurate and relevant answers.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Retrieval-augmented approach to LLM applications. Image by author.</p> <p>When a user asks a question, an intelligent search tool looks for relevant information in the provided Knowledge bases. For example, you might have encountered instances of searching for relevant information within PDFs or a company's documentation. Most of those examples use vector similarity search to identify which chunks of text might contain relevant data to answer the user's question accurately. The implementation is relatively straightforward.</p> <p>{width=\"6.268055555555556in\" height=\"3.825in\"}</p> <p>RAG applications using vector similarity search. Image by author.</p> <p>The PDFs or the documentation are first split into multiple chunks of text. Some different strategies include how large the text chunks should be and if there should be any overlap between them. In the next step, vector representations of text chunks are generated by using any of the available text embedding models. That is all the preprocessing needed to perform a vector similarity search at query time. The only step left is to encode the user input as a vector at query time and use cosine or any other similarity to compare the distance between the user input and the embedded text chunks. Most frequently, you will see that the top three most similar documents are returned to provide the context to the LLM to enhance its capability to generate accurate answers. This approach works fairly well when the vector search can produce relevant chunks of text.</p> <p>However, simple vector similarity search might not be sufficient when the LLM needs information from multiple documents or even just multiple chunks to generate an answer.</p> <p>For example, consider the following question:</p> <p>Did any of the former OpenAI employees start their own company?</p> <p>If you think about it, this question can be broken down into two questions.</p> <ul> <li> <p>Who are the former employees of OpenAI?</p> </li> <li> <p>Did any of them start their own company?</p> </li> </ul> <p>{width=\"4.65in\" height=\"4.125in\"}</p> <p>Information spanning across multiple documents. Image by author.</p> <p>Answering these types of questions is a\u00a0multi-hop question-answering task, where a single question can be broken down into multiple sub-questions and can require numerous documents to be provided to the LLM to generate an accurate answer.</p> <p>The above-mentioned workflow of simply chunking and embeddings documents in a database and then using plain vector similarity search might struggle with multi-hop questions due to:</p> <ul> <li> <p>Repeated information in top N documents: The provided documents     are not guaranteed to contain complementary and complete information     needed to answer a question. For example, the top three similar     documents might all mention that\u00a0Shariq\u00a0worked at\u00a0OpenAI\u00a0and     possibly founded a company while completely ignoring all the other     former employees that became founders</p> </li> <li> <p>Missing reference information:\u00a0Depending on the chunk sizes, you     might lose the reference to the entities in the documents. This can     be partially solved by chunk overlaps. However, there are also     examples where the references point to another document, so some     sort of co-reference resolution or other preprocessing would be     needed.</p> </li> <li> <p>Hard to define ideal N number of retrieved documents: Some     questions require more documents to be provided to an LLM to     accurately answer the question, while in other situations, a large     number of provided documents would only increase the noise (and     cost).</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"3.9256944444444444in\"}</p> <p>An example where the similarity search might return some duplicated information, while other relevant information could be ignored due to a low K number of retrieved information or embedding distance. Image by the author.</p> <p>Therefore, a plain vector similarity search might struggle with multi-hop questions. However, we can employ multiple strategies to attempt to answer multi-hop questions requiring information from various documents.</p> <p>Knowledge Graph as Condensed Information Storage</p> <p>If you are paying close attention to the LLM space, you might have come across the idea of using various techniques to condense information for it to be more easily accessible during query time. For example, you\u00a0[could use an LLM to provide a summary of documents]{.underline}\u00a0and then embed and store the summaries instead of the actual documents. Using this approach, you could remove a lot of noise, get better results, and worry less about prompt token space.</p> <p>Interestingly, you could conduct the contextual summarization at ingestion or\u00a0[perform it during the query time]{.underline}. Contextual compression during query time is interesting as the context is picked that is relevant to the provided question, so it is a bit more guided. However, the heavier the workload during the query time, the worse the expected user latency will be. Therefore, it is recommended to move as much of the workload to ingestion time as possible to improve latency and avoid other runtime issues.</p> <p>The same approach can be applied to\u00a0[summarize conversation history]{.underline}\u00a0to avoid running into\u00a0token limit problems.</p> <p>I haven't seen any articles about combining and summarizing multiple documents as a single record. The problem is probably that there are too many combinations of documents that we could merge and summarize. Therefore, it is perhaps\u00a0too costly to process all the combinations\u00a0of documents at ingestion time.\\ However, a knowledge graph can help here too.</p> <p>The process of extracting structured information in the form of entities and relationships from unstructured text has been around for some time and is better known as\u00a0[the information extraction pipeline]{.underline}. The beauty of combining an information extraction pipeline with knowledge graphs is that you can process each document individually, and the information from different records gets connected when the knowledge graph is constructed or enriched.</p> <p>{width=\"6.268055555555556in\" height=\"4.863888888888889in\"}</p> <p>Extracting entities and relationships from text to construct a knowledge graph. Image by author.</p> <p>The knowledge graph used nodes and relationships to represent data. In this example, the first document provided the information that\u00a0Dario\u00a0and\u00a0Daniela\u00a0used to work at\u00a0OpenAI, while the second document offered information about their\u00a0Anthropic\u00a0startup. Each record was processed individually, yet\u00a0the knowledge graph representation connects the data\u00a0and makes it easy to answer questions spanning across multiple documents.</p> <p>Most of the newer approaches using LLMs to answer multi-hop questions we encountered focus on solving the task at query time. However, we believe that many multi-hop question-answering issues can be solved by preprocessing data before ingestion and connecting it in a knowledge graph. The information extraction pipeline can be\u00a0[performed using LLMs]{.underline}\u00a0or\u00a0[custom text domain models]{.underline}.</p> <p>In order to retrieve information from the knowledge graph at query time, we have to construct an appropriate Cypher statement. Luckily, LLMs are pretty good at translating natural language to Cypher graph-query language.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Using knowledge graphs as part of retrieval-augmented LLM applications. Image by author.</p> <p>In this example, the smart search uses an LLM to generate an appropriate Cypher statement to retrieve relevant information from a knowledge graph. The relevant information is then passed to another LLM call, which uses the original question and the provided information to generate an answer. In practice, you could use different LLMs for generating Cypher statements and answers or use various prompts on a single LLM.</p> <p>Combining Graph and Textual Data</p> <p>Sometimes, you might want to combine textual and graph data to find relevant information. For example, consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p>In this example, you might want to identify the Prosper Robotics founders using the knowledge graph structure and retrieve the latest articles mentioning them.</p> <p>{width=\"6.268055555555556in\" height=\"6.795833333333333in\"}</p> <p>Knowledge graph with explicit links between structured information and unstructured text. Image by author.</p> <p>To answer the question about the latest news about Prosper Robotics founders, you would start from the Prosper Robotics node, traverse to its founders, and then retrieve the latest articles mentioning them.</p> <p>A knowledge graph can be used to represent structured information about entities and their relationships, as well as unstructured text as node properties.\u00a0Additionally, you could employ natural language techniques like named entity recognition to connect unstructured information to relevant entities in the knowledge graph, as shown with the\u00a0MENTIONS\u00a0relationship.</p> <p>We believe that the future of retrieval-augmented generation applications is utilizing both structured and unstructured information to generate accurate answers. Therefore, a knowledge graph is a perfect solution because you can store both structured and unstructured data and connect them with explicit relationships, making information more accessible and easier to find.</p> <p>{width=\"6.268055555555556in\" height=\"3.1875in\"}</p> <p>Using Cypher and vector similarity search to retrieve relevant information from a knowledge graph. Image by author.</p> <p>When the knowledge graph contains structured and unstructured data, the smart search tool could utilize Cypher queries or vector similarity search to retrieve relevant information. In some cases, you could also use a combination of the two. For example, you could start with a Cypher query to identify relevant documents and then use vector similarity search to find specific information within those documents.</p> <p>Using Knowledge Graphs in Chain-of-Thought Flow</p> <p>Another very exciting development around LLMs is the so-called\u00a0[chain-of-thought question answering]{.underline}, especially with\u00a0[LLM agents]{.underline}. The idea behind LLM agents is that they can decompose questions into multiple steps, define a plan, and use any of the provided tools. In most cases, the agent tools are APIs or knowledge bases that the agent can access to retrieve additional information. Let's again consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p>{width=\"6.268055555555556in\" height=\"6.801388888888889in\"}</p> <p>No explicit links between knowledge graph entities and unstructured text. Image by author.</p> <p>Suppose you don't have explicit connections between articles and entities they mention. The articles and entities could even be in separate databases. In this case, an LLM agent using chain-of-thought flow would be very helpful. First, the agent would decompose the question into sub-questions.</p> <ul> <li> <p>Who are the founders of Prosper Robotics?</p> </li> <li> <p>What is the latest news about them?</p> </li> </ul> <p>Now, an agent could decide which tool to use. Suppose we provide it with a knowledge graph access that it can use to retrieve structured information. Therefore, an agent could choose to retrieve the information about the founders of Prosper Robotics from a knowledge graph. As we already know, the founder of Prosper Robotics is Shariq Hashme. Now that the first question was answered, the agent could rewrite the second subquestion as:</p> <ul> <li>What is the latest news about Shariq Hashme?</li> </ul> <p>The agent could use any of the available tools to answer the subsequent question. The tools can range from knowledge graphs, document or vector databases, various APIs, and more. Having access to structured information allows LLM applications to perform various analytics workflows where aggregation, filtering, or sorting is required. Consider the following questions:</p> <ul> <li> <p>Which company with a solo founder has the highest valuation?</p> </li> <li> <p>Who founded the most companies?</p> </li> </ul> <p>Plain vector similarity search can struggle with these types of analytical questions since it searches through unstructured text data, making it hard to sort or aggregate data. Therefore, a combination of structured and unstructured data is probably the future of retrieval-augmented LLM applications. Additionally, as we have seen, knowledge graphs are also ideal for representing connected information and, consequently, multi-hop queries.</p> <p>While the chain-of-thought is a fascinating development around LLMs as it shows how an LLM can reason, it is not the most user-friendly as the response latency can be high due to multiple LLM calls. However, we are still very excited to understand more about incorporating knowledge graphs into chain-of-thought flows for various use cases.</p> <p>Summary</p> <p>Retrieval-augmented generation applications often require retrieving information from multiple sources to generate accurate answers. While textual summarization can be challenging, representing information in a graph format can offer several advantages.</p> <p>By processing each document separately and connecting them in a knowledge graph, we can construct a structured representation of the information. This approach allows for easier traversal and navigation through interconnected documents, enabling multi-hop reasoning to answer complex queries. Furthermore, constructing the knowledge graph during the ingestion phase reduces the workload during query time, resulting in improved latency.</p> <p>Another advantage of using a knowledge graph is its ability to store both structured and unstructured information. This flexibility makes a\u00a0[knowledge graphs suitable]{.underline}\u00a0for a wide range of language model (LLM) applications, as it can handle various data types and relationships between entities. The graph structure provides a visual representation of the knowledge, facilitating transparency and interpretability for both developers and users.</p> <p>Overall, leveraging knowledge graphs in retrieval-augmented generation applications offers benefits such as improved query efficiency, multi-hop reasoning capabilities, and support for structured and unstructured information.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our\u00a0[GitHub repository]{.underline}.</p> <p>GitHub - neo4j/NaLLM: Repository for the NaLLM project</p> <p>Repository for the NaLLM project. Contribute to neo4j/NaLLM development by creating an account on GitHub.</p> <p>github.com</p>"},{"location":"Real-Time%20Graph%20Analytics/","title":"Real Time Graph Analytics","text":"<p>Knowledge Graphs &amp; LLMs: Real-Time Graph Analytics</p> <p>Understanding data points through the context of their relationships</p> <p>This is the fourth blog post of Neo4j's NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can read the previous three blog posts here:</p> <ul> <li> <p>[Harnessing LLMs with     Neo4j]{.underline}</p> </li> <li> <p>[Fine-tuning vs. Retrieval-augmented     generation]{.underline}</p> </li> <li> <p>[Multi-hop Question     Answering]{.underline}</p> </li> </ul> <p>{width=\"6.268055555555556in\" height=\"6.268055555555556in\"}</p> <p>Graph data analyst as imagined by Midjourney.</p> <p>Large Language Models (LLMs) have significantly changed data accessibility to the average person. Less than a year ago, accessing the company's data required technical skills involving proficiency in numerous dashboarding tools or even diving into the intricacies of a database query language. Yet, with the rise of LLMs like ChatGPT, the wealth of knowledge hidden within private databases or accessible via various APIs is now more readily available than ever with the rise of so-called retrieval-augmented LLM applications.</p> <p>{width=\"6.268055555555556in\" height=\"3.1305555555555555in\"}</p> <p>Retrieval-augmented generation application. Image by author.</p> <p>The idea behind retrieval-augmented applications is to retrieve additional information from various sources to allow the LLM to generate better and more accurate results. It seems OpenAI has also picked up on this trend as they\u00a0[introduced OpenAI\u00a0functions]{.underline}\u00a0recently. The new OpenAI models are trained to use provide parameters to functions (or what other libraries call\u00a0[tools]{.underline}), whose signatures and descriptions are passed in the context, to retrieve additional information at query time if needed.</p> <p>We have observed a strong bias for vector similarity search in retrieval-augmented applications. If you opened Twitter or LinkedIn in the past three months, you might have seen the various \"Chat with your PDFs\" applications. In those examples, the implementation is relatively straightforward. The text is extracted from PDFs, split into chunks if needed, and finally stored in a vector database along with its text embedding representations.</p> <p>The barrier to entry with these types of applications is low, especially if you are dealing with small amounts of data. It is fascinating that so many\u00a0[articles giving the impression that only vector databases are relevant]{.underline}\u00a0for retrieval-augmented applications are published nowadays.</p> <p>While there is immense power in vector similarity-based information retrieval from unstructured text, we believe that structured information has an important role to play in LLM applications.</p> <p>Last time we wrote about\u00a0[multi-hop question answering]{.underline}\u00a0and how knowledge graphs can help solve problems of retrieving information from multiple documents to generate an accurate answer. Additionally, we hinted that vector similarity search is not designed for analytics workflows, where we rely on structured information.</p> <p>Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering</p> <p>Retrieve information that spans across multiple documents</p> <p>medium.com</p> <p>For example, questions like:</p> <ul> <li> <p>Who could introduce me to Emil Eifr\u00e9m (CEO of Neo4j)?</p> </li> <li> <p>How is ALOX5 gene related to Crohn's disease?</p> </li> <li> <p>When we have a particular microservice outage, how does it affect     our products?</p> </li> <li> <p>How does a flight delay propagate through the network?</p> </li> <li> <p>Which users can be credited for a social media post virality?</p> </li> </ul> <p>All these questions require highly-connected information to be able to answer the question accurately. For example, to learn who can introduce you to Emil, you need information about relationships between people.</p> <p>On the other hand, you need to map dependencies between your microservices and products in order to evaluate the scale and severity of a particular microservice failure.</p> <p>In this blog post, we will introduce some of the\u00a0frequent use cases of real-time graph analytics\u00a0that you might want to implement into your LLM applications.</p> <p>Finding (Shortest) Paths</p> <p>Relationships are first-class citizens in native graph databases. Although knowledge graphs allow you to perform typical aggregations and filtering to answer questions like \"How many customers did we get this week?\", we will focus more on analytical use cases where traversing the relationships is the main component. One such example is finding the shortest or all possible paths between data points. For example, to answer the question:</p> <p>Who could introduce me to Emil Eifrem?</p> <p>We would have to find the shortest path between myself and Emil Eifrem in the graph.</p> <p>{width=\"6.268055555555556in\" height=\"3.9972222222222222in\"}</p> <p>Single shortest path. Image by author.</p> <p>Another use case where finding the shortest paths in real-time would come in handy is in any sort of transportation, logistics, or routing application. In these applications, you might want to evaluate the top N shortest paths to ensure some fallback plan if something unexpected happens.</p> <p>{width=\"6.268055555555556in\" height=\"3.7104166666666667in\"}</p> <p>Top two shortest paths between stops in Rome. Image by author.</p> <p>This image visualizes the top 2 shortest paths between two stops in Rome. Such shortest paths could be optimized for distance, time, cost, or a combination.</p> <p>Another domain where finding paths between data points in your LLM applications is the biomedical domain. In the biomedical domain, you are dealing with genes, proteins, diseases, drugs, and more. What's perhaps more important is that these entities do not exist in isolation but have complex, often multilayered relationships with each other.</p> <p>For instance, a gene may be associated with multiple diseases, a protein may interact with numerous other proteins, a disease might be treatable by a variety of drugs, and a drug could have multiple effects on different genes and proteins.</p> <p>Given the staggering amount of biomedical data available, the number of potential relationships between these data points is enormous and is a great candidate to be represented as a knowledge graph.</p> <p>{width=\"6.268055555555556in\" height=\"5.9743055555555555in\"}</p> <p>All shortest paths between Crohn's disease and ALOX5 gene using\u00a0[Hetionet dataset]{.underline}. Image by author.</p> <p>Biomedical knowledge graph can support LLM applications where users are be interested in answering questions like</p> <p>How is ALOX5 gene related to Crohn's disease?</p> <p>While most LLM applications we see today generate answers as a natural language, there is also an excellent opportunity for returning responses in the form of line, bar, or even network visualizations. Often the LLM can even return the configuration structure needed for the charting libraries.</p> <p>Information Propagating Through Network</p> <p>Another strong knowledge graph fit is domains with networks of dependencies. For example, you could have a knowledge graph containing the complete\u00a0[microservice architecture of your system]{.underline}. Such a knowledge graph would allow you to power a DevOps chatbot that would enable you to evaluate the architecture in real-time and perform what-if analysis.</p> <p>{width=\"6.268055555555556in\" height=\"3.839583333333333in\"}</p> <p>Microservices &amp; People graph. Screenshot from\u00a0[https://www.youtube.com/watch?v=_qakAUjXiek&amp;t=2517s]{.underline}</p> <p>Also Rhys Evans presented how the\u00a0[Financial Times manages their infrastructure as a graph]{.underline}.</p> <p>Another domain that comes to mind is the supply chain.</p> <p>{width=\"6.268055555555556in\" height=\"3.359722222222222in\"}</p> <p>Image from\u00a0[https://neo4j.com/blog/graphs-in-automotive-and-manufacturing/]{.underline}.</p> <p>Incorporating\u00a0[supply chain data into knowledge graphs]{.underline}\u00a0can significantly enhance the capabilities of large language applications. This approach allows us to structure complex supply chain information into nodes and relationships, thereby generating a holistic picture of how materials, components, and products flow from suppliers to customers. The inherent interconnections and dependencies become evident and analyzable.</p> <p>For language applications, this enables deeper context understanding and knowledge generation. For instance, an AI model like ChatGPT can leverage this data structure to produce more accurate and insightful responses about supply chain scenarios, disruptions, or management strategies. It could comprehend and explain the ripple effects of a shortage of a certain component, predict potential bottlenecks, or suggest optimization strategies.</p> <p>By aligning the intricacies of supply chain dynamics with the cognitive abilities of AI, we can bolster the functionality and value of large language applications in a multitude of industrial and commercial contexts.</p> <p>Social Network Analysis and Data Science</p> <p>What if your company chatbot went beyond documentation and helped deliver insights and recommendations as part of the people analytics?</p> <p>{width=\"6.268055555555556in\" height=\"4.699305555555555in\"}</p> <p>Image from\u00a0[https://neo4j.com/blog/neo4j-critical-aspect-human-capital-management-hcm/]{.underline}.</p> <p>[Knowledge graphs in HCM]{.underline}\u00a0can serve as an invaluable tool in driving people analytics within a company, primarily by creating a robust, interconnected system of information that allows for a deep, holistic understanding of employee behavior, skills, competencies, interactions, and performance. Essentially, a knowledge graph captures and links complex employee data --- including demographic information, role history, project involvement, performance indicators, and skillsets --- allowing for multi-faceted analysis.</p> <p>This combination of connected data and ML powered tools enable human resources and team leaders to uncover hidden patterns, identify high-potential individuals, predict future performance, assess skill gaps, and inform training needs, thereby fueling data-driven decision-making. By leveraging a knowledge graph, companies can streamline talent management and development processes, enhancing overall organizational effectiveness and fostering a culture of continuous learning and improvement.</p> <p>Incorporating a chatbot interface into this knowledge graph-driven people analytics system could revolutionize the way companies approach HR and talent management. Here's how:</p> <p>User-Friendly Access to Complex Data</p> <p>A chatbot interface provides an intuitive, conversational manner for users to interact with complex datasets. Employees, managers, or HR staff wouldn't need to understand intricate databases or analytics tools; they could simply ask the chatbot questions about employee performance, skills, or team dynamics.</p> <p>The chatbot, equipped with natural language processing capabilities, would interpret the question, retrieve the relevant information from the knowledge graph, and deliver the response in an understandable format.</p> <p>Real-Time Insights</p> <p>The chatbot interface could offer immediate access to data insights, enabling timely decisions.</p> <p>If a manager wanted to know how many projects are in the pipeline and which people are a good fit and available for a specific project, they could ask the chatbot and get an answer in real-time rather than waiting for a comprehensive report.</p> <p>Scalable Training and Support</p> <p>The chatbot could provide individualized support to employees, answering questions about company policies, procedures, or career development opportunities.</p> <p>It could even deliver personalized training recommendations (and perhaps the actual training itself) based on an individual's role, skills, and career goals. This would democratize access to learning and development resources, making it easier for employees to upskill or reskill.</p> <p>Predictive Analysis</p> <p>Advanced AI chatbots could analyze patterns and trends from the knowledge graph to make predictions, such as which employees might be at risk of leaving the company or what skills may be in demand in the future. These predictive analytics capabilities could help companies be proactive rather than reactive in their HR strategies.</p> <p>In essence, integrating a chatbot interface with a knowledge graph-driven people analytics system would make complex employee data more accessible, actionable, and useful to all members of an organization. It would be a game-changer in talent management and development, driving a more data-informed, proactive, and personalized approach to HR.</p> <p>Summary</p> <p>In conclusion, as we traverse deeper into the era of large language models, we must keep in mind the\u00a0enormous potential of knowledge graphs in structuring, organizing, and retrieving information\u00a0in these applications. The combination of structured and unstructured data retrieval paves the way for more accurate, reliable, and impactful results, extending beyond natural language answers into the realm of visually represented information.</p> <p>Despite the popularity of vector similarity-based data retrieval (recall), we should not underestimate the role of structured information and the immense value it brings to LLM applications. Whether it's finding the shortest paths, understanding complex biomedical relationships, analyzing supply chain scenarios, or revolutionizing HR with people analytics, the applications of knowledge graphs are vast and profound. We believe the future of LLM-based applications is the combination of vector similarity search approach coupled with database query languages such as Cypher.</p> <p>Through this blog post, we've explored some exciting real-time graph analytics use cases that could be implemented into your LLM applications. This is only the beginning. We anticipate a future where large language models will work more cohesively with knowledge graphs, bringing about more innovative solutions to real-world problems.</p>"}]}